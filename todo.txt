CONTENT OF THE REPO:
    1. Report (write in LaTeX and then convert in PDF)
    2. Datasets:
        - all MONK datasets;
        - ML-CUP training (whole portion of the dataset for which we have labels)
        - ML-CUP test (blind test set)
        - ML-CUP test internal (i.e., the portion of ML-CUP training that we use as internal test set)
        - ML-CUP "dev set" (i.e., the complementary of ML-CUP test internal divided into several datasets
            for doing hold-out, k-fold etc.)
    3. Short Abstract for ML-CUP (as described in slides ML-22-PRJ-v.0.11.pdf)

EXTRA PARTS (beyond standard MLP with backprop, as described in slides) that we could (easily) implement:
    1. Nesterov momentum (I'll need it for CM)
    2. L1, L1L2 regularization (L1 is also for CM, I've already implemented it)
    3. [maybe] also R-Prop or Adam as optimizers
    4. compare different strategies for learning rate (I've implemented some of them in schedulers.py)

EXPERIMENTAL PART:
    1. All 3 MONK problems with performance and learning curves
    2. For the MONK problems, we must use a network with 2-4 units: in the slides it is said that 1-of-k
    encoding leads to 17 units, so we need to solve this
    3. Use different styles for plotting curves in each graph (they will print them out in black-and-white)


IMPLEMENTATION:
DONE:
    1. Add Input Layer (output is equal to input, for loading data and being consistent for how NNs have been presented
    in the course)

    4. Refactor tests in main.py to tests package [(partially) DONE]

    8. Add methods to save and load models (this is for model selection, after training on multiple models we would be able
    to save the best one, both for MONK benchmarks and for ML CUP) [(partially) DONE]
    # For the above, we do not save weight updates state (including momentums), so it cannot be used e.g. for resuming
    # training

    3. Add metrics (MEE, accuracy, Root MSE, time) [(partially) DONE]
    # For the above, time is missing (needs to be added as callback and recorded or as a metric with separate handling)
    # There are no tests involving metrics up to now, but History class seems to work
    # Validation values for metrics are missing

    10. Add logging facilities: for each epoch (and maybe also for each minibatch), we want to monitor:
        1. training data and regularization loss (average over minibatches for epochs);
        2. validation data and regularization loss (as above);
        3. all other metrics passed in model.compile(...), e.g. MEE, Root MSE, time
    [(partially) DONE]
    # 3. is okay, 2. is missing, for 1. it is printed the average of the sum of data and regularization loss
    # Maybe we should disentangle them ('data_loss', 'reg_loss', 'val_data_loss', 'val_reg_loss')

    11. Add Callbacks for stopping criterions:
        - standard stop (no improvement in training) [with regularization];
        - EarlyStopping;
        - any other stopping criterion in the slides;
    OR adapt the sklearn ones (in this case, we shall conform to keras and sklearn interfaces
    and by now we are not compliant with it).
    [(partially) DONE]
    # The only ones implemented are ModelCheckpoint, ModelBackup, TrainCSVLogger, TestCSVLogger, History
    # Need to implement 'StandardStop', EarlyStopping
    # Need to make tests for them


PRIORITY:

    REPLACE np.reshape WITH np.expand_dims

    14. Checkout course lectures if there is something missing

    0.1. Complete metrics:
        - Add timing (minibatch/epoch, as callback and/or pluggable in csv logger)

        - Update metric recording s.t. it is possible to record in the History also validation metrics

        - Add tests for checking vals (in particular, test for MSE checking id produces identical values when
        len(dataset) is a multiple of mb_size)

        - Check if "already-reduced" MSE is okay/better than the present one (as metric)

    0.2. Complete callbacks (notes above)

    0.3. Complete logging (notes above)

    5. Add logic for converting OUR Layers/Losses/Optimizers/Models etc. into keras ones for 'auto-testing' of the
    produced models

UNDONE:
    1b. (add also Linear Activation Layer?)

    2. Check CrossEntropy Loss and Softmax with (even) a small dataset to see what happens when training/validating
    (e.g. MONKs)

    6. Add Fan-in initialization (basically, extend RandomUniformInitializer or directly pass np.sqrt(#units) to it)

    7. Write down Dataset for MONK benchmarks and for ML CUP

    9. Write any utility for preprocessing data (e.g. for normalizing them or any other necessary transformation);
    in particular, this applies to MONK and ML-CUP datasets

    12. Add "data split and train" routines:
        - Hold-out: start from a bunch of data and create three dataset (train, validation, test), then train and evaluate
        the resulting model;
        - k-fold CV: start from a bunch of data and create k different datasets, then at each time merge k-1 of them to use
        for training and use the remaining for validation, cycling on the k datasets for validation; [k = 5, 10 w.r.t. LOO];

    13. Add "hyperparameter tuning" routines:
        - Grid-Search: start from a set of possible hyperparameters, then train and evaluate the models for each combination;
        - [se si riesce] Nested Grid Search: start with a coarse-grained grid search, then refine it on good parameters
    NOTE: For 12) and 13), these should be "parameters", i.e. we want to select one from 12) and one from 13) into a bigger
    algorithm that performs all the trainings and evaluations and selects the best model;