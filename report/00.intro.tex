\section{Introduction}
The project consists in the implementation and testing from scratch of a framework for building Artificial Neural Networks with backpropagation
algorithm for both classification and regression problems, providing the essential building blocks for defining the structure of a network, loss
functions, optimizers, metrics to monitor the performances of the models and callbacks for customizing behaviour in a simple way. Our objective
was to try to find the best model for both MONK and ML-CUP22 tasks by using traditional Grid Search with Holdout split for development set and
a subsequent k-fold on the best models. The most performant one is then used to predict outputs for the blind test set. 

\section{Method}

\subsection{Code}

The framework has been implemented in Python using numpy for the computational part, some minor utilities from sklearn and pandas and the
pickle library for serialization; tensorflow is used in several tests for comparing our results with an industry-standard tool.
Finally, the typer library is used to build a command-line interface.
For our implementation, we took inspiration from both Tensorflow and PyTorch frameworks, and in particular we made an interface
that is quite similar to keras API for its clarity, modularity and ease of use and for faster prototyping and comparisons.

\paragraph{}

The core package contains the implementation of the framework, while the tests one contains several tests with scikit-learn datasets
and with MONK and CUP datasets, together with comparison tests with tensorflow. The neural network is implemented in the Model class,
which contains a sequence of Layer objects and exposes the compile() and train() methods for training, predict() for test and save() and
load() for serialization. Layer class is subclassed by Dense, Linear, Tanh, Sigmoid, ReLU and SoftmaxLayer: Dense implements a fully-connected
layer, while the last four implement their correspondent activation functions; finally, Linear implements a layer with no activation function
and is equivalent to a Dense layer with identity as activation. In particular, Dense objects are the composition of a Linear and an Activation
object: this choice is inspired by the PyTorch API and is intended to give more flexibility without having to explicitly define a Dense layer
with identity activation.
The optimizer provided is the standard SGD with momentum, while L1, L2 and L1L2 techniques are available for regularization. Provided losses
are MSE and Cross Entropy. Training can be done either in batch, online or minibatch mode by using the DataLoader class. Weights initialization
is available with Normal and Uniform distribution and with fan-in and fan-out techniques.

\paragraph{}

The framework provides a system of metrics and callbacks that can be plugged in the model for customizing behaviour: in particular, metrics
are provided together with optimizer and loss by calling compile(), while callbacks have a series of methods that are called before and after
each training epoch/batch and each validation step that can be implemented. Examples of callbacks include EarlyStopping, loggers and the History
class, while available metrics are Mean Squared Error, Mean Absolute Error, Root MSE, Mean Euclidean Error, Binary and Categorical Accuracy.

\paragraph{}

Finally, the core.search package provides the base classes BaseSearch and Validator, which define the base structure for search strategies and
validation techniques (including splits for training and validation sets) and are subclassed by GridSearch, FixedCombSearch, Holdout and KFold.
Search can optionally be done in parallel by using up to all the available cores.

\subsection{Validation Scheme}

 * data split is 90% development set and 10% internal test set; dev set is split in 75% training and 25% validation when using Holdout;
 * we performed an initial Grid Search on several architectures (9-8-8-2, 9-16-8-2, 9-16-16-2) and then a 5-fold cross-validation on the best 25%
 of the models according to validation MEE after the first search
 * before training and at the end of each epoch we shuffle the current training dataset
 <complete>

\subsection{Trials Pursued}

Monks started working with 100\% accuracy immediately after adopting one-hot encoding, we had just to pay some attention to regularization for the 3rd set.

\paragraph{}
Cup dataset was trickier and it required us to try the heuristics described above (and in deeper details in experiments section) first manually trying to guess the right order of magnitude for parameters and then with a more exhaustive grid search. Once found the best possible NN model we tried to slightly modify our approach trying to filter out the noise: we trained a batch of 20 networks separately and ranked them for increasing loss on validation, took the bests and averaged their feed-forward results on test set assessing that this performed slightly better than each one taken separately. In the end we tried the pairwise products input preprocessing, combined with the previous heuristic, finding that it performed again slightly better. 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End: